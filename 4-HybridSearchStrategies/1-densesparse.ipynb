{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d885e245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sumanyadav/Desktop/Learn/Learn-RAG/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91ca4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Sample Documnets\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain is a framework for building applications with LLMs.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for sematic search\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"langchain has many types of retrievers\")\n",
    "]\n",
    "\n",
    "# step 2 - Dense Retrieve (FAISS + HuggingFace)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6c0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sparse Retriever\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k = 3\n",
    "\n",
    "\n",
    "# step 4 - Combine with Ensamble Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weight=[0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba9d6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x176dac1a0>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x176dac830>, k=3)], weights=[0.5, 0.5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72bc1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='06145013-8e42-42ad-990f-68394b8acb5f', metadata={}, page_content='Langchain can be used to develop agentic ai application.'), Document(id='1d594fc6-5c79-4f21-90dd-c1f7d897634e', metadata={}, page_content='langchain has many types of retrievers'), Document(id='555776f0-d971-4dbb-90ec-fff0ff8547ac', metadata={}, page_content='LangChain is a framework for building applications with LLMs.'), Document(id='a719b496-e1b8-460f-8b5a-d8be3de52153', metadata={}, page_content='Pinecone is a vector database for sematic search'), Document(metadata={}, page_content='The Eiffel Tower is located in Paris.')]\n",
      "\n",
      "Doument 1: Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Doument 2: langchain has many types of retrievers\n",
      "\n",
      "Doument 3: LangChain is a framework for building applications with LLMs.\n",
      "\n",
      "Doument 4: Pinecone is a vector database for sematic search\n",
      "\n",
      "Doument 5: The Eiffel Tower is located in Paris.\n"
     ]
    }
   ],
   "source": [
    "# step 5 - Query and get result\n",
    "query = \"How can i build an application using LLMs?\"\n",
    "\n",
    "results = hybrid_retriever.invoke(query)\n",
    "print(results)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nDoument {i+1}: {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f6d73",
   "metadata": {},
   "source": [
    "# RAG PIPELINE WITH HYBRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a8722fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eefe669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the following context:\\ncontext: {context}\\nQuestion: {input}\\n')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6 - prompt template\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the following context:\n",
    "context: {context}\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5256085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 16385, 'max_output_tokens': 4096, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': False, 'structured_output': False, 'image_url_inputs': False, 'pdf_inputs': False, 'pdf_tool_message': False, 'image_tool_message': False, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x14b38ab10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x14b38aea0>, root_client=<openai.OpenAI object at 0x14b38a780>, root_async_client=<openai.AsyncOpenAI object at 0x14b38ac40>, temperature=0.4, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 7 - LLMs\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.4\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad2fe9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x176dac1a0>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x176dac830>, k=3)], weights=[0.5, 0.5]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the following context:\\ncontext: {context}\\nQuestion: {input}\\n')\n",
       "            | ChatOpenAI(profile={'max_input_tokens': 16385, 'max_output_tokens': 4096, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': False, 'structured_output': False, 'image_url_inputs': False, 'pdf_inputs': False, 'pdf_tool_message': False, 'image_tool_message': False, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x14b38ab10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x14b38aea0>, root_client=<openai.OpenAI object at 0x14b38a780>, root_async_client=<openai.AsyncOpenAI object at 0x14b38ac40>, temperature=0.4, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 8 - Ragchain\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever=hybrid_retriever, combine_docs_chain=document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bdb5c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: You can build an application using LLMs by using the LangChain framework, which is specifically designed for developing applications with Large Language Models (LLMs). LangChain provides various types of retrievers that can be used to interact with LLMs and create agentic AI applications.\n",
      "\n",
      "Document 1: Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Document 2: langchain has many types of retrievers\n",
      "\n",
      "Document 3: LangChain is a framework for building applications with LLMs.\n",
      "\n",
      "Document 4: Pinecone is a vector database for sematic search\n",
      "\n",
      "Document 5: The Eiffel Tower is located in Paris.\n"
     ]
    }
   ],
   "source": [
    "query = {\"input\": \"How can I build an application using LLMs?\"}\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "print(f\"Answer: {response['answer']}\")\n",
    "\n",
    "for i, doc in enumerate(response['context']):\n",
    "    print(f\"\\nDocument {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773d45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learn-RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
