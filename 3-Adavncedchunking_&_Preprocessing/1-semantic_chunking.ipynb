{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6e540b",
   "metadata": {},
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f4def7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sumanyadav/Desktop/Learn/Learn-RAG/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363f58c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic chunk \n",
      "\n",
      "Chunk 1\n",
      "Chunk: LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "\n",
      "Chunk 2\n",
      "Chunk: You can create chains, agents, memory, and retrievers.\n",
      "\n",
      "Chunk 3\n",
      "Chunk: The Eiffel Tower is located in Paris.\n",
      "\n",
      "Chunk 4\n",
      "Chunk: France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# sample text\n",
    "text =\"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "# step 1 - split the sentences\n",
    "sentences = [s.strip() for s in text.split('\\n') if s.strip()]\n",
    "\n",
    "# step 2- embed each sentence\n",
    "embeddings=model.encode(sentences)\n",
    "\n",
    "# step 3 - Initialize parameters\n",
    "threshold = 0.7\n",
    "chunks = []\n",
    "current_chunk = [sentences[0]]\n",
    "\n",
    "# step 4 - Semantic grouping based on threshold\n",
    "for i in range(1, len(sentences)):\n",
    "    similarity = cosine_similarity(\n",
    "        [embeddings[i-1]],\n",
    "        [embeddings[i]]\n",
    "    )[0][0]\n",
    "\n",
    "    if similarity>=threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "        current_chunk=[sentences[i]]\n",
    "    \n",
    "chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# output\n",
    "print(\"\\nSemantic chunk \")\n",
    "for index, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {index+1}\\nChunk: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8befa90",
   "metadata": {},
   "source": [
    "\n",
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d249bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c491f11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62084f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Semantic chunk\n",
    "\n",
    "class ThresholdSemanticChunker:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', threshold=0.7):\n",
    "        self.model=SentenceTransformer(model_name)\n",
    "        self.threshold=threshold\n",
    "    \n",
    "    def split(self, text:str):\n",
    "        sentences = [s.strip() for s in text.split('\\n') if s.strip()]\n",
    "        embeddings=model.encode(sentences)\n",
    "        threshold = 0.7\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "        for i in range(1, len(sentences)):\n",
    "            similarity = cosine_similarity(\n",
    "                [embeddings[i-1]],\n",
    "                [embeddings[i]]\n",
    "            )[0][0]\n",
    "\n",
    "            if similarity>=threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk=[sentences[i]]\n",
    "    \n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        return chunks\n",
    "    \n",
    "    def split_documents(self, docs):\n",
    "        result=[]\n",
    "        for doc in docs:\n",
    "            for chunk_text in self.split(doc.page_content):\n",
    "                result.append(\n",
    "                    Document(\n",
    "                        page_content=chunk_text,\n",
    "                        metadata=doc.metadata \n",
    "                    )\n",
    "                )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9072700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='\\nLangChain is a framework for building applications with LLMs.\\nLangchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\\nYou can create chains, agents, memory, and retrievers.\\nThe Eiffel Tower is located in Paris.\\nFrance is a popular tourist destination.\\n')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text =\"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "doc = Document(page_content=sample_text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2298c730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.'),\n",
       " Document(metadata={}, page_content='You can create chains, agents, memory, and retrievers.'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris.'),\n",
       " Document(metadata={}, page_content='France is a popular tourist destination.')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## chunking\n",
    "chunker = ThresholdSemanticChunker()\n",
    "chunks = chunker.split_documents([doc])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbe7f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b53271ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Answere the question based on the following context'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Context:\\n{context}\\n\\nQuestion:\\n{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PROMPT TEMPLATE\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answere the question based on the following context\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98756a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document chain\n",
    "# LLM\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    #model=\"groq:gemma2-9b-it\",\n",
    "    temperature=0.4\n",
    ")\n",
    "\n",
    "# RAG chain (NO document chain needed)\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": lambda x: x\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8991f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for building applications with large language models (LLMs). It provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone, allowing users to create chains, agents, memory, and retrievers.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangChain used for?\"\n",
    "answer = rag_chain.invoke(query)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a115cd",
   "metadata": {},
   "source": [
    "## Semantic chunker using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d413b43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk number: 1\n",
      "chunk preview: LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "metadata: {'source': 'langchain_intro.txt'}\n",
      "chunk number: 2\n",
      "chunk preview: You can create chains, agents, memory, and retrievers. The Eiffel Tower is located in Paris. France is a popular tourist destination.\n",
      "metadata: {'source': 'langchain_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "## Load Documents\n",
    "loader = TextLoader(\"langchain_intro.txt\")\n",
    "docs=loader.load()\n",
    "\n",
    "## initialize embedding model\n",
    "embeddings=OpenAIEmbeddings()\n",
    "\n",
    "## Create the semantic chunker\n",
    "chunker = SemanticChunker(embeddings)\n",
    "\n",
    "## split the documents\n",
    "chunks = chunker.split_documents(docs)\n",
    "\n",
    "# result\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'chunk number: {i+1}')\n",
    "    print(f'chunk preview: {chunk.page_content}')\n",
    "    print(f'metadata: {chunk.metadata}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c63df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learn-RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
